---
title:  "Chapter 01: Prediction as a unifying theme in statistics and causal inference"
author: "Craig Slinkman"
date: "11/14/2020"
output: html_document
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Chapter 1: Overview

This [book][1] explores the challenges of building, understanding, and using predictive models.

## 1.1 The three challenges of statistics

The three challenges of [statistical inference][2] are: 

1. Generalizing from sample to population, a problem that is associated with survey sampling but  actually arises in nearly every application of statistical inference.

2. Generalizing from treatment to control group, a problem that is associated with causal inference,  which is implicitly or explicitly part of the interpretation of most regressions we have seen.

3. Generalizing from observed measurements to the underlying constructs of interest, as most of the  time our data do not record exactly what we would ideally like to study. 

All three of these challenges can be framed as problems of prediction (for new people or new items  that are not in the sample, future outcomes under different potentially assigned treatments, and  underlying constructs of interest, if they could be measured exactly).  The key skills you should learn from this book are:  

* Understanding regression models. These are mathematical models for predicting an outcome  variable from a set of predictors, starting with straight-line fits and moving to various nonlinear  generalizations.  

* Constructing regression models. The regression framework is open-ended, with many options  involving the choice of what variables to include and how to transform and constrain them.  

* Fitting regression models to data, which we do using the open-source software R and Stan.  

* Displaying and interpreting the results, which requires additional programming skills and  mathematical understanding.  A central subject of this book.

## Why learn regression?

[Regression][3] is a method that allows researchers to summarize how predictions or average values of  Elections  and the  economy  an outcome vary across individuals defined by a set of predictors. For example, Figure 1.1a shows  the incumbent party’s vote share in a series of U.S. presidential elections, plotted vs. a measure of  economic growth in the period leading up to each election year. Figure 1.1b shows a linear regression  fit to these data. The model allows us to predict the vote—with some uncertainty—given the economy  and under the assumption that future elections are in some way like the past. 

[3] Gelman, Andrew; Hill, Jennifer; Vehtari, Aki. Regression and Other Stories (Analytical Methods for Social Research) (p. 4). Cambridge University Press. Kindle Edition. 








## Load required directories

Note we will use the Bayesian modeling language call $stan$.  $stan$ is a probabilistic programming language for statistical inference written in C++. The Stan language is used to specify a (Bayesian) statistical model with an imperative program calculating the log probability density function. Stan is licensed under the New BSD License.  

We also use the R-package the $arm$ package.  The $arm $ contains R functions for Bayesian inference using $lm$, $glm$, $mer$ and $polr$ objects. BACCO is an R bundle for Bayesian analysis of random functions. BACCO contains three sub-packages: $emulator%, $calibrator$, and $approximator$, that perform Bayesian emulation and calibration of computer programs.
  
These two packages are loaded below.  You will need to install these packages before you run this code.

```{r}
require( tidyverse )                    # I live in the tidyverse ...   
require( readr )                        # To read table objects ...
require( rstanarm )                     # For Bayesian approach to statistics ...
require( arm )                          # Additional Bayesian functionality ...
require( cowplot )                      # For more profesional graphics ...
library("bayesplot")                    # For plotting Bayesian analysis ...
theme_set(bayesplot::theme_default(base_family = "sans"))
```


## 1.2 Why learn regression

Note that I converted the hiss.dat file to a csv file.

### Reading the hibbs election data onto a tibble

```{r}
fp <- file.path( getwd(),              # Specify full path to read file ...
                 "data",
                 "hibbs.csv" )
hibbs <- read_csv( fp )                # Read the file

hibbs                                  # Verify file
```

## Plot data showing year of election 

```{r}
pl <-  
hibbs %>% 
  ggplot( aes( x =growth, y = vote, label = year )) +
    geom_text( ) +
  xlab( "Average growth in personel income" ) +
  ylab( "Incumbent's party share of the votes") +
  ggtitle( "Forecasting the election from the economy")

slr <- lm( vote ~ growth, data = hibbs )
beta <- slr$coefficients

pr <- 
  hibbs %>% 
    ggplot( aes( x =growth, y = vote, label = year )) +
    geom_point() +
    geom_smooth(method ="lm", se=FALSE ) +
    annotate('text', x = 2, y =47, 
        label = "hat(y) == beta[1]",
        parse = TRUE,size=10) +
    xlab( "Average growth in personel income" ) +
    ylab( "Incumbent's party share of the votes") +
    ggtitle( "Data and linear fit")
pr
```

# References

[1] Gelman, Andrew; Hill, Jennifer; Vehtari, Aki. Regression and Other Stories (Analytical Methods for Social Research) (p. 3). Cambridge University Press. Kindle Edition. 

[2]Gelman, Andrew; Hill, Jennifer; Vehtari, Aki. Regression and Other Stories (Analytical Methods for Social Research) (p. 3). Cambridge University Press. Kindle Edition.

[3] Gelman, Andrew; Hill, Jennifer; Vehtari, Aki. Regression and Other Stories (Analytical Methods for Social Research) (p. 4). Cambridge University Press. Kindle Edition. 
