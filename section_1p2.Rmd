---
title: "Section 1.2"
author: "Andrew Gelmaan, at  all as implemnted by Craig Slinkman"
date: "12/8/2020"
header:-includes:
  - \usepackage{amsfonts}
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


Regression[1] is a method that allows researchers to summarize how predictions or average values of outcome vary across individuals defined by a set of predictors.

I live in the tidyverse and use ggplot2 to draw ggplot2 to draw my graphs.

## Loading required packages

I live in the tidyverse and use ggplot2 to draw ggplot2 to draw my graphs.

We also need to load the stand packages so we can estimate the posterior distribution using stan.

```{r}
library( tidyverse )                     # I live in the tidyverse ...
library( readr )                         # To read data in the tidyverse ...
library( rstan )                         # For Monte Carlo simulation ...
library(rstanarm )                       # For Bayesian applied regression modeling ...
library( cowplot )                       # For additional graphics functionality ...
```

## Read the hibbs data

I have downloaded the data from the text website at at www.stat.columbia.edu/~gelman/regression/.
It is my practice to store the data for a project in the data subdirectory of the R-project.  I then build a file path to the file directory.  We do this below.


```{r}
cd <- getwd()                           # Get current directory ...

fp <-                                   # Construct file path to data.
  file.path( cd,
             "data",
             "hibbs.csv" )

hibbs <- read_csv( fp )                 # Read data as tibble ...

hibbs                                   # Verify hibbs tibble
```

## Draw Figure 1.1a

```{r}
figure_01.1a <-   
    ggplot( hibbs,
          aes( x = growth,
               y = vote,
               label = year ) ) +
    geom_text() +
    scale_x_continuous( name   = "Average recent growth in personal income",
                        labels = scales::percent_format(accuracy = 1,
                                                        scale    = 1.0)) +
    scale_y_continuous( name   = "Incumbet party's vote share",
                        labels = scales::percent_format( accuracy = 1,
                                                         scale    = 1)) +
    ggtitle( "Forecasting incumbets vote share from the economy" ) +
    theme_cowplot()

figure_01.1a
```

## Figure 1.1b

My first version is just to plot the regression points.  I will add the
estimated regression line latter to show how one can add layers to a plot
using ggplot.

### Just the points

```{r}
figure_01.1b <- 
  ggplot( hibbs,
          aes( x = growth, y = vote )) +
    geom_point() +
     scale_x_continuous( name   = "Average recent growth in personal income",
                        labels = scales::percent_format(accuracy = 1,
                                                        scale    = 1.0)) +
    scale_y_continuous( name   = "Incumbet party's vote share",
                        labels = scales::percent_format( accuracy = 1,
                                                         scale    = 1)) +
    ggtitle( "Forecasting incumbets vote share from the economy" ) +
    theme_cowplot()

figure_01.1b
```


### Estimating the regression function using Monte Carlo methos

In this text we will not use classical estimation techniques but will Bayesian 
estimation techniques statistics.  For most of the models we can't are unable to use the conjugate prior method to estimate the posterior distribution. We will use Markov Chain Monte Carlo [2]approach to estimate the posterior distribution.

>Markov chain Monte Carlo (MCMC) methods comprise a class of algorithms for sampling from a probability distribution. By constructing a Markov chain that has the desired distribution as its equilibrium distribution, one can obtain a sample of the desired distribution by recording states from the chain. The more steps that are included, the more closely the distribution of the sample matches the actual desired distribution. Various algorithms exist for constructing chains, including the Metropolisâ€“Hastings algorithm.

We use the R-package **stanarm** to estimate the posterior distribution.  According to the the stand organization[3] the package  

>**rstanarm** is an R package that emulates other R model-fitting functions but uses Stan (via the rstan package) for the back-end estimation. The primary target audience is people who would be open to Bayesian inference if using Bayesian software were easier but would use frequentist software otherwise.

>Fitting models with rstanarm is also useful for experienced Bayesian software users who want to take advantage of the pre-compiled Stan programs that are written by Stan developers and carefully implemented to prioritize numerical stability and the avoidance of sampling problems.

You will need to install these packages and load this library to compute the posterior distribution. The packages that we will use are **rstan** and **rstanarm**. 

We will now fir the data using the **R** function **stan_glm**. 

```{r}
bayes <-                                # Fit regression using stan ...
  stan_glm( vote ~growth, 
            data = hibbs )

bayes                                   # Display regression coefficients ...

beta <- coef( bayes )                   # Extract the regression coefficients ...

```

Our estimated regression equation is:

$$y = `r round( beta[1], 2)` + `r round(beta[2], 2)` x$$

Now that we have the estimated regression coefficients or rather the medians of the posterior distributions of the regression coefficients we can add a line to* **figure_01.1b** as follows:
```{r}
figure_01.1b <-                         # Add regression line to scatterplot ...
  figure_01.1b +
  geom_abline( intercept = beta[1],
               slope     = beta[2])

figure_01.1b                            # Add Bayes regression line to scatterplot ...
```

If we want to compare compare the the Bayes estimates to the ordinary least squares (OLS) estimates we can do this as follows.  We will fit both models and add the predicted values for each estimate to the hibbs data.

```{r}
Bayes <- fitted( bayes )                # Save fitted values

ols <- lm( vote ~ growth,               # Fit the OLS model ...
           data = hibbs )

beta2 <- coef( ols  )                   # Extract the OLS coefficients

OLS <- fitted( ols )                    # Save OLS fitted values ...

################################################################################
# We create a tibble that will be used to draw the scatterplot, and add the
# fitted regression lines for each method.
################################################################################

fitted <- add_column( hibbs,   Bayes )
fitted <- add_column( fitted, OLS )

fitted 
                                        # Verify columns added ...


################################################################################
# The current tibble is in wide form format as each data value of the fitted
# values is in a separate column.  We will used the R-function pivot_longer to
# put the fitted tibble in long form where the y-data values will be in a 
# column and and the source of each value is recorded in a new column.
################################################################################

long_form <-                           # Convert to long form
  pivot_longer( fitted,
                cols = c( Bayes, OLS ),
                names_to = "Fit",
                values_to = "Fitted" )

long_form                               # Confirm long_form  

            
################################################################################
# We can know plot the both regression lines on our scatter plot as follows.
################################################################################

ggplot( long_form,
        aes( x = growth, 
             y = vote )) +
  geom_point() +
  geom_line( aes( x = growth, 
                  y =  Fitted, 
                  color = Fit )) +
  scale_x_continuous( name   = "Average recent growth in personal income",
                        labels = scales::percent_format(accuracy = 1,
                                                        scale    = 1.0)) +
  scale_y_continuous( name   = "Incumbet party's vote share",
                        labels = scales::percent_format( accuracy = 1,
                                                         scale    = 1)) +
    ggtitle( "Comparison of Bayes and OLS Fits" ) +
    theme_cowplot()
    

```
### The three approaches to mesuring uncertainity 

There are three interpretation of probability[4].  They are the the classical theory of probability terpreination, the relative frequency interpretation, and the subjective probability interpretation.  The mathematical probability take the same form with all of the interpretations.

#### Classical probabiligty

Classical probability[5] that event E occurs is denoted by P(E). When all outcomes are equally likely. The peobability that the event $E$ occurs is given by:

$$Pr(E) = \frac{the number\ of\ occurences\ of\ E}{number\ o\ possible\ outcomes}$$
To use this interpretation you need to be able to compute or the number of outcomes.  It doesnot have much practical use other than in computing outcomes in games such as pokewr and craps. 



## References

[1] Gelman, Andrew; Hill, Jennifer; Vehtari, Aki. Regression and Other Stories (Analytical Methods for Social Research) (p. 4). Cambridge University Press. Kindle Edition. 

[2] Wikipedia, https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo.  Accessed December 10, 2020 as 11:30.  

[3] Stan Organization, http://mc-stan.org/rstanarm/, Accessed on December 10, 2020 at 11:50.

[4] Penn State University Department of Statistics,  Statistical Notes Leeson 2, https://online.stat.psu.edu/stat500/lesson/2/2.3, accessed on 12/12/2020 at 12:23.

[5] ibid, accessed on December 10, 2020 at 13:07.

[6] 